#!/usr/bin/env python3
"""
OpenAI LLMã‚’ä½¿ç”¨ã—ãŸã‚²ãƒ¼ãƒ ç†è«–å®Ÿé¨“

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯å®Ÿéš›ã«OpenAI GPT-4ã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®
æˆ¦ç•¥çš„ç›¸äº’ä½œç”¨ã‚’å®Ÿè¡Œã—ã€è©³ç´°ãªä¼šè©±å±¥æ­´ã¨æ¨è«–éç¨‹ã‚’è¨˜éŒ²ã—ã¾ã™ã€‚
"""

import asyncio
import os
import sys
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
from dataclasses import dataclass

# å¿…è¦ãªä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from openai import AsyncOpenAI
from dotenv import load_dotenv

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


@dataclass
class SimpleAgentAction:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¡Œå‹•"""
    COOPERATE = "COOPERATE"
    DEFECT = "DEFECT"
    SHARE_KNOWLEDGE = "SHARE_KNOWLEDGE"
    WITHHOLD_KNOWLEDGE = "WITHHOLD_KNOWLEDGE"


@dataclass
class SimpleGameDecision:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚²ãƒ¼ãƒ æ±ºå®š"""
    action: str
    reasoning: str
    confidence: float
    knowledge_to_share: List[str] = None


class SimpleLLMAgent:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªLLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆç‹¬ç«‹å®Ÿè£…ï¼‰"""
    
    def __init__(self, name: str, strategy: str, model: str = "gpt-4o-mini"):
        self.name = name
        self.strategy = strategy
        self.model = model
        self.client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.interaction_history = []
        self.trust_scores = {}
        
        print(f"âœ… LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ '{name}' ã‚’ä½œæˆ (æˆ¦ç•¥: {strategy}, ãƒ¢ãƒ‡ãƒ«: {model})")
    
    async def make_decision(self, game_context: Dict[str, Any]) -> SimpleGameDecision:
        """LLMã‚’ä½¿ç”¨ã—ã¦æ„æ€æ±ºå®šã‚’è¡Œã†"""
        
        try:
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            prompt = self._build_prompt(game_context)
            
            print(f"ğŸ¤” {self.name} ãŒæ€è€ƒä¸­...")
            
            # OpenAI APIã‚’å‘¼ã³å‡ºã—
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": self._get_system_instructions()},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                max_tokens=500,
                response_format={"type": "json_object"}
            )
            
            # å¿œç­”ã‚’è§£æ
            content = response.choices[0].message.content
            print(f"ğŸ’­ {self.name} ã®å¿œç­”: {content[:100]}...")
            
            decision_data = json.loads(content)
            
            # æ±ºå®šã‚’ãƒ­ã‚°
            decision = SimpleGameDecision(
                action=decision_data.get("action", "DEFECT"),
                reasoning=decision_data.get("reasoning", "ç†ç”±ãªã—"),
                confidence=float(decision_data.get("confidence", 0.5)),
                knowledge_to_share=decision_data.get("knowledge_to_share", [])
            )
            
            print(f"ğŸ¯ {self.name} ã®æ±ºå®š: {decision.action} (ä¿¡é ¼åº¦: {decision.confidence:.2f})")
            print(f"ğŸ“ ç†ç”±: {decision.reasoning[:80]}...")
            
            return decision
            
        except Exception as e:
            print(f"âŒ {self.name} ã®LLMå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ±ºå®š
            return self._fallback_decision()
    
    def _get_system_instructions(self) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã‚’å–å¾—"""
        
        base_instructions = f"""
ã‚ãªãŸã¯'{self.name}'ã¨ã„ã†åå‰ã®ã‚²ãƒ¼ãƒ ç†è«–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚
æˆ¦ç•¥: {self.strategy}

ã‚²ãƒ¼ãƒ ç†è«–çš„ç›¸äº’ä½œç”¨ã«ãŠã„ã¦æˆ¦ç•¥çš„æ„æ€æ±ºå®šã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

åˆ©ç”¨å¯èƒ½ãªè¡Œå‹•:
- COOPERATE: å”åŠ›ã™ã‚‹
- DEFECT: è£åˆ‡ã‚‹/ç«¶äº‰ã™ã‚‹
- SHARE_KNOWLEDGE: çŸ¥è­˜ã‚’å…±æœ‰ã™ã‚‹
- WITHHOLD_KNOWLEDGE: çŸ¥è­˜ã‚’ç§˜åŒ¿ã™ã‚‹

å¿…ãšJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{
    "action": "COOPERATE|DEFECT|SHARE_KNOWLEDGE|WITHHOLD_KNOWLEDGE",
    "reasoning": "è©³ç´°ãªæ¨è«–éç¨‹ã‚’æ—¥æœ¬èªã§èª¬æ˜",
    "confidence": 0.0ã‹ã‚‰1.0ã®ä¿¡é ¼åº¦,
    "knowledge_to_share": ["å…±æœ‰ã™ã‚‹çŸ¥è­˜ã®ãƒªã‚¹ãƒˆ"] ã¾ãŸã¯ []
}}
"""
        
        # æˆ¦ç•¥åˆ¥æŒ‡ç¤º
        strategy_instructions = {
            "cooperative": "å¸¸ã«å”åŠ›çš„ã§ä¿¡é ¼ã§ãã‚‹è¡Œå‹•ã‚’å–ã‚Šã€é•·æœŸçš„é–¢ä¿‚ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚",
            "competitive": "å€‹äººã®åˆ©ç›Šã‚’æœ€å¤§åŒ–ã—ã€æˆ¦ç•¥çš„ã«ç›¸æ‰‹ã‚ˆã‚Šå„ªä½ã«ç«‹ã¤ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„ã€‚",
            "tit_for_tat": "åˆå›ã¯å”åŠ›ã—ã€ãã®å¾Œã¯ç›¸æ‰‹ã®å‰å›è¡Œå‹•ã‚’çœŸä¼¼ã—ã¦ãã ã•ã„ã€‚",
            "adaptive": "ç›¸æ‰‹ã®è¡Œå‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€çŠ¶æ³ã«å¿œã˜ã¦æœ€é©ãªæˆ¦ç•¥ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚",
            "random": "äºˆæ¸¬ä¸å¯èƒ½ãªè¡Œå‹•ã‚’å–ã‚Šã¾ã™ãŒã€ãã‚Œã§ã‚‚è«–ç†çš„ãªç†ç”±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        }
        
        specific_instruction = strategy_instructions.get(self.strategy, "")
        
        return base_instructions + "\n\næˆ¦ç•¥ã‚¬ã‚¤ãƒ‰: " + specific_instruction
    
    def _build_prompt(self, game_context: Dict[str, Any]) -> str:
        """æ„æ€æ±ºå®šç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        
        parts = []
        parts.append("## ç¾åœ¨ã®çŠ¶æ³")
        parts.append(f"ã‚²ãƒ¼ãƒ : {game_context.get('game_type', 'ä¸æ˜')}")
        parts.append(f"ãƒ©ã‚¦ãƒ³ãƒ‰: {game_context.get('round', 1)}")
        
        if 'opponent' in game_context:
            parts.append(f"ç›¸æ‰‹: {game_context['opponent']}")
        
        if 'opponent_last_action' in game_context:
            parts.append(f"ç›¸æ‰‹ã®å‰å›è¡Œå‹•: {game_context['opponent_last_action']}")
        
        if 'my_history' in game_context:
            parts.append(f"ç§ã®è¡Œå‹•å±¥æ­´: {game_context['my_history']}")
        
        if 'opponent_history' in game_context:
            parts.append(f"ç›¸æ‰‹ã®è¡Œå‹•å±¥æ­´: {game_context['opponent_history']}")
        
        parts.append("\nä¸Šè¨˜ã®æƒ…å ±ã‚’åŸºã«ã€ã‚ãªãŸã®æˆ¦ç•¥ã«å¾“ã£ã¦æœ€é©ãªè¡Œå‹•ã‚’æ±ºå®šã—ã¦ãã ã•ã„ã€‚")
        
        return "\n".join(parts)
    
    def _fallback_decision(self) -> SimpleGameDecision:
        """ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ±ºå®š"""
        if self.strategy == "cooperative":
            action = "COOPERATE"
        elif self.strategy == "competitive":
            action = "DEFECT"
        else:
            action = "COOPERATE"
        
        return SimpleGameDecision(
            action=action,
            reasoning=f"LLMã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ({self.strategy}æˆ¦ç•¥)",
            confidence=0.3
        )
    
    def update_history(self, my_action: str, opponent_action: str, payoff: float):
        """ç›¸äº’ä½œç”¨å±¥æ­´ã‚’æ›´æ–°"""
        self.interaction_history.append({
            "my_action": my_action,
            "opponent_action": opponent_action,
            "payoff": payoff
        })


class SimpleGameRunner:
    """ã‚·ãƒ³ãƒ—ãƒ«ãªã‚²ãƒ¼ãƒ å®Ÿè¡Œå™¨"""
    
    def __init__(self):
        self.results = []
    
    async def run_prisoners_dilemma(self, agent1: SimpleLLMAgent, agent2: SimpleLLMAgent, rounds: int = 3):
        """å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒã‚’å®Ÿè¡Œ"""
        
        print(f"\nğŸ® å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒã‚²ãƒ¼ãƒ é–‹å§‹: {agent1.name} vs {agent2.name}")
        print(f"ãƒ©ã‚¦ãƒ³ãƒ‰æ•°: {rounds}")
        
        game_result = {
            "game_type": "prisoners_dilemma",
            "players": [agent1.name, agent2.name],
            "rounds": [],
            "total_payoffs": {agent1.name: 0, agent2.name: 0}
        }
        
        agent1_history = []
        agent2_history = []
        
        for round_num in range(1, rounds + 1):
            print(f"\n--- ãƒ©ã‚¦ãƒ³ãƒ‰ {round_num} ---")
            
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ„æ€æ±ºå®š
            context1 = {
                "game_type": "prisoners_dilemma",
                "round": round_num,
                "opponent": agent2.name,
                "opponent_history": agent2_history,
                "my_history": agent1_history
            }
            
            context2 = {
                "game_type": "prisoners_dilemma", 
                "round": round_num,
                "opponent": agent1.name,
                "opponent_history": agent1_history,
                "my_history": agent2_history
            }
            
            # åŒæ™‚æ„æ€æ±ºå®š
            decision1, decision2 = await asyncio.gather(
                agent1.make_decision(context1),
                agent2.make_decision(context2)
            )
            
            # å ±é…¬è¨ˆç®—
            payoff1, payoff2 = self._calculate_payoffs(decision1.action, decision2.action)
            
            # çµæœè¨˜éŒ²
            round_result = {
                "round": round_num,
                "actions": {agent1.name: decision1.action, agent2.name: decision2.action},
                "payoffs": {agent1.name: payoff1, agent2.name: payoff2},
                "reasoning": {
                    agent1.name: decision1.reasoning,
                    agent2.name: decision2.reasoning
                },
                "confidence": {
                    agent1.name: decision1.confidence,
                    agent2.name: decision2.confidence
                }
            }
            
            game_result["rounds"].append(round_result)
            game_result["total_payoffs"][agent1.name] += payoff1
            game_result["total_payoffs"][agent2.name] += payoff2
            
            # å±¥æ­´æ›´æ–°
            agent1_history.append(decision1.action)
            agent2_history.append(decision2.action)
            
            agent1.update_history(decision1.action, decision2.action, payoff1)
            agent2.update_history(decision2.action, decision1.action, payoff2)
            
            print(f"ğŸ² çµæœ: {agent1.name}={decision1.action} ({payoff1}), {agent2.name}={decision2.action} ({payoff2})")
        
        # æœ€çµ‚çµæœ
        winner = max(game_result["total_payoffs"], key=game_result["total_payoffs"].get)
        game_result["winner"] = winner
        
        print(f"\nğŸ† ã‚²ãƒ¼ãƒ çµ‚äº†!")
        print(f"å‹è€…: {winner}")
        print(f"æœ€çµ‚ã‚¹ã‚³ã‚¢: {game_result['total_payoffs']}")
        
        self.results.append(game_result)
        return game_result
    
    def _calculate_payoffs(self, action1: str, action2: str) -> tuple:
        """å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒã®å ±é…¬ã‚’è¨ˆç®—"""
        
        # åŸºæœ¬çš„ãªå›šäººã®ã‚¸ãƒ¬ãƒ³ãƒãƒãƒˆãƒªãƒƒã‚¯ã‚¹
        if action1 == "COOPERATE" and action2 == "COOPERATE":
            return (3, 3)  # ç›¸äº’å”åŠ›
        elif action1 == "COOPERATE" and action2 == "DEFECT":
            return (0, 5)  # è£åˆ‡ã‚‰ã‚Œã‚‹
        elif action1 == "DEFECT" and action2 == "COOPERATE":
            return (5, 0)  # è£åˆ‡ã‚‹
        else:  # DEFECT and DEFECT
            return (1, 1)  # ç›¸äº’è£åˆ‡ã‚Š


async def run_llm_experiment():
    """LLMå®Ÿé¨“ã®ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    print("ğŸ¤– OpenAI LLMã‚²ãƒ¼ãƒ ç†è«–å®Ÿé¨“")
    print("=" * 40)
    
    # APIã‚­ãƒ¼ç¢ºèª
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    print(f"âœ… OpenAI APIã‚­ãƒ¼ç¢ºèªæ¸ˆã¿")
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ
    print("\nğŸ§  LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆä¸­...")
    
    agents = [
        SimpleLLMAgent("å”åŠ›å¤ªéƒ", "cooperative"),
        SimpleLLMAgent("ç«¶äº‰èŠ±å­", "competitive"),
        SimpleLLMAgent("æˆ¦ç•¥æ¬¡éƒ", "tit_for_tat"),
        SimpleLLMAgent("é©å¿œç¾é¦™", "adaptive")
    ]
    
    # ã‚²ãƒ¼ãƒ å®Ÿè¡Œå™¨ã‚’ä½œæˆ
    game_runner = SimpleGameRunner()
    
    # ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚²ãƒ¼ãƒ ã‚’å®Ÿè¡Œ
    print(f"\nğŸ¯ ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚²ãƒ¼ãƒ ã‚’å®Ÿè¡Œä¸­...")
    
    total_games = 0
    for i in range(len(agents)):
        for j in range(i + 1, len(agents)):
            total_games += 1
            agent1, agent2 = agents[i], agents[j]
            
            print(f"\nğŸ“Š ã‚²ãƒ¼ãƒ  {total_games}: {agent1.name} vs {agent2.name}")
            
            try:
                result = await game_runner.run_prisoners_dilemma(agent1, agent2, rounds=3)
                
                # è©³ç´°åˆ†æ
                print(f"\nğŸ“ˆ è©³ç´°åˆ†æ:")
                for round_data in result["rounds"]:
                    print(f"  ãƒ©ã‚¦ãƒ³ãƒ‰{round_data['round']}:")
                    for agent_name, reasoning in round_data["reasoning"].items():
                        confidence = round_data["confidence"][agent_name]
                        action = round_data["actions"][agent_name]
                        print(f"    {agent_name}: {action} (ä¿¡é ¼åº¦:{confidence:.2f})")
                        print(f"      æ¨è«–: {reasoning[:60]}...")
                
            except Exception as e:
                print(f"âŒ ã‚²ãƒ¼ãƒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue
    
    # å…¨ä½“çµæœåˆ†æ
    print(f"\nğŸ‰ å®Ÿé¨“å®Œäº†ï¼å…¨{total_games}ã‚²ãƒ¼ãƒ å®Ÿè¡Œ")
    await analyze_results(game_runner.results, agents)
    
    # çµæœä¿å­˜
    save_results(game_runner.results, agents)


async def analyze_results(results: List[Dict], agents: List[SimpleLLMAgent]):
    """çµæœã®è©³ç´°åˆ†æ"""
    
    print(f"\nğŸ“Š å®Ÿé¨“çµæœã®ç·åˆåˆ†æ")
    print("=" * 30)
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåˆ¥çµ±è¨ˆ
    agent_stats = {}
    for agent in agents:
        agent_stats[agent.name] = {
            "total_payoff": 0,
            "games_played": 0,
            "games_won": 0,
            "cooperation_count": 0,
            "total_actions": 0
        }
    
    # çµ±è¨ˆè¨ˆç®—
    for result in results:
        for agent_name in result["players"]:
            stats = agent_stats[agent_name]
            stats["total_payoff"] += result["total_payoffs"][agent_name]
            stats["games_played"] += 1
            
            if result["winner"] == agent_name:
                stats["games_won"] += 1
            
            # å”åŠ›ç‡è¨ˆç®—
            for round_data in result["rounds"]:
                action = round_data["actions"][agent_name]
                stats["total_actions"] += 1
                if action == "COOPERATE":
                    stats["cooperation_count"] += 1
    
    # çµæœè¡¨ç¤º
    print(f"ğŸ† ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæˆç¸¾ãƒ©ãƒ³ã‚­ãƒ³ã‚°:")
    sorted_agents = sorted(agent_stats.items(), key=lambda x: x[1]["total_payoff"], reverse=True)
    
    for rank, (agent_name, stats) in enumerate(sorted_agents, 1):
        avg_payoff = stats["total_payoff"] / max(stats["games_played"], 1)
        win_rate = stats["games_won"] / max(stats["games_played"], 1)
        cooperation_rate = stats["cooperation_count"] / max(stats["total_actions"], 1)
        
        print(f"  {rank}ä½. {agent_name}:")
        print(f"    - å¹³å‡å ±é…¬: {avg_payoff:.2f}")
        print(f"    - å‹ç‡: {win_rate:.3f}")
        print(f"    - å”åŠ›ç‡: {cooperation_rate:.3f}")
        print(f"    - ç·ã‚²ãƒ¼ãƒ æ•°: {stats['games_played']}")
    
    # æˆ¦ç•¥åˆ†æ
    print(f"\nğŸ§  æˆ¦ç•¥åŠ¹æœåˆ†æ:")
    strategy_performance = {}
    for agent in agents:
        strategy = agent.strategy
        stats = agent_stats[agent.name]
        
        if strategy not in strategy_performance:
            strategy_performance[strategy] = []
        
        avg_payoff = stats["total_payoff"] / max(stats["games_played"], 1)
        cooperation_rate = stats["cooperation_count"] / max(stats["total_actions"], 1)
        
        strategy_performance[strategy].append({
            "avg_payoff": avg_payoff,
            "cooperation_rate": cooperation_rate
        })
    
    for strategy, performances in strategy_performance.items():
        avg_payoff = sum(p["avg_payoff"] for p in performances) / len(performances)
        avg_cooperation = sum(p["cooperation_rate"] for p in performances) / len(performances)
        
        print(f"  {strategy}æˆ¦ç•¥:")
        print(f"    - å¹³å‡å ±é…¬: {avg_payoff:.2f}")
        print(f"    - å¹³å‡å”åŠ›ç‡: {avg_cooperation:.3f}")


def save_results(results: List[Dict], agents: List[SimpleLLMAgent]):
    """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_dir = Path("llm_experiment_results")
    results_dir.mkdir(exist_ok=True)
    
    # è©³ç´°çµæœ
    detailed_results = {
        "timestamp": timestamp,
        "agents": [{"name": a.name, "strategy": a.strategy} for a in agents],
        "games": results
    }
    
    results_file = results_dir / f"llm_experiment_{timestamp}.json"
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {results_file}")
    
    # ä¼šè©±ãƒ­ã‚°ã®ä¿å­˜
    conversation_file = results_dir / f"conversations_{timestamp}.txt"
    with open(conversation_file, 'w', encoding='utf-8') as f:
        f.write(f"OpenAI LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¼šè©±ãƒ­ã‚°\n")
        f.write(f"å®Ÿé¨“æ™‚åˆ»: {timestamp}\n")
        f.write("=" * 50 + "\n\n")
        
        for i, result in enumerate(results, 1):
            f.write(f"ã‚²ãƒ¼ãƒ  {i}: {' vs '.join(result['players'])}\n")
            f.write("-" * 30 + "\n")
            
            for round_data in result["rounds"]:
                f.write(f"\nãƒ©ã‚¦ãƒ³ãƒ‰ {round_data['round']}:\n")
                for agent_name, reasoning in round_data["reasoning"].items():
                    action = round_data["actions"][agent_name]
                    confidence = round_data["confidence"][agent_name]
                    f.write(f"{agent_name}: {action} (ä¿¡é ¼åº¦: {confidence:.2f})\n")
                    f.write(f"æ¨è«–: {reasoning}\n\n")
            
            f.write(f"æœ€çµ‚çµæœ: {result['total_payoffs']}\n")
            f.write(f"å‹è€…: {result['winner']}\n\n")
            f.write("=" * 50 + "\n\n")
    
    print(f"ğŸ’¬ ä¼šè©±ãƒ­ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {conversation_file}")


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    print("ğŸš€ OpenAI LLMã‚²ãƒ¼ãƒ ç†è«–å®Ÿé¨“ã‚·ã‚¹ãƒ†ãƒ ")
    print("å®Ÿéš›ã®GPT-4o-miniã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®æˆ¦ç•¥çš„ç›¸äº’ä½œç”¨ã‚’å®Ÿè¡Œã—ã¾ã™")
    
    # å®Ÿè¡Œç¢ºèª
    response = input("\nå®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ(APIæ–™é‡‘ãŒç™ºç”Ÿã—ã¾ã™) (y/N): ")
    if response.lower() != 'y':
        print("å®Ÿé¨“ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return
    
    await run_llm_experiment()
    
    print("\nğŸ‰ LLMå®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    asyncio.run(main())