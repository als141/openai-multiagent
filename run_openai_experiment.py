#!/usr/bin/env python
"""OpenAI Agents SDKã‚’ä½¿ç”¨ã—ãŸãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿé¨“ã®å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ"""

import asyncio
import sys
import os
import argparse
from pathlib import Path
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

from src.experiments.openai_multi_agent_experiment import MultiAgentExperiment
from src.agents.openai_game_agent import GameTheoryAgent, CoordinatorAgent
from src.game_theory.strategies import Strategy
from src.game_theory.games import GameType


async def run_quick_demo():
    """ç°¡å˜ãªãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ"""
    print("ğŸš€ OpenAI Agents SDK ãƒ‡ãƒ¢ã‚’é–‹å§‹")
    
    # 2ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã®ç°¡å˜ãªå®Ÿé¨“
    agents = [
        GameTheoryAgent("Alice", Strategy.COOPERATIVE, "å”åŠ›çš„ã§å„ªã—ã„"),
        GameTheoryAgent("Bob", Strategy.TIT_FOR_TAT, "å…¬å¹³ã‚’é‡è¦–ã™ã‚‹")
    ]
    
    experiment = MultiAgentExperiment("quick_demo")
    
    # å›šäººã®ã‚¸ãƒ¬ãƒ³ãƒã‚’5ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ
    await experiment.run_game_theory_interaction(agents, GameType.PRISONERS_DILEMMA, rounds=5)
    
    # çŸ¥è­˜å…±æœ‰ã‚»ãƒƒã‚·ãƒ§ãƒ³
    coordinator = CoordinatorAgent(managed_agents=agents)
    await experiment.run_knowledge_sharing_session(
        agents, coordinator,
        "åŠ¹ç‡çš„ãªãƒãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ§‹ç¯‰æ–¹æ³•"
    )
    
    experiment._save_results()
    print("âœ… ãƒ‡ãƒ¢å®Œäº†")


async def run_custom_experiment(
    agent_count: int,
    game_rounds: int,
    problems: List[str]
):
    """ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    print(f"ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ã‚’é–‹å§‹ (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°: {agent_count})")
    
    # æŒ‡å®šã•ã‚ŒãŸæ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
    strategies = [Strategy.COOPERATIVE, Strategy.COMPETITIVE, Strategy.TIT_FOR_TAT, 
                 Strategy.ADAPTIVE, Strategy.RANDOM]
    agents = []
    
    for i in range(agent_count):
        strategy = strategies[i % len(strategies)]
        personality = ["å”åŠ›çš„", "ç«¶äº‰çš„", "å…¬å¹³", "æŸ”è»Ÿ", "å‰µé€ çš„"][i % 5]
        agent = GameTheoryAgent(
            name=f"Agent_{i+1}",
            strategy=strategy,
            personality=f"{personality}ãªæ€§æ ¼ã‚’æŒã¤"
        )
        agents.append(agent)
    
    experiment = MultiAgentExperiment("custom_experiment")
    
    # ã‚²ãƒ¼ãƒ ç†è«–çš„ç›¸äº’ä½œç”¨
    for game_type in [GameType.PRISONERS_DILEMMA, GameType.PUBLIC_GOODS]:
        await experiment.run_game_theory_interaction(agents, game_type, rounds=game_rounds)
    
    # å•é¡Œè§£æ±º
    coordinator = CoordinatorAgent(managed_agents=agents)
    for problem in problems:
        await experiment.run_complex_problem_solving(agents, coordinator, problem)
    
    experiment._analyze_final_trust_network(agents)
    experiment._save_results()
    print("âœ… ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“å®Œäº†")


async def run_benchmark_experiment():
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ã‚’å®Ÿè¡Œ"""
    print("ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ã‚’é–‹å§‹")
    
    experiment = MultiAgentExperiment("benchmark_experiment")
    
    # æ¨™æº–çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚»ãƒƒãƒˆã‚’ä½œæˆ
    agents = experiment.create_diverse_agents()
    coordinator = CoordinatorAgent(managed_agents=agents)
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œ
    benchmark_problems = [
        # å¤šè§’çš„è¦–ç‚¹ãŒå¿…è¦ãªå•é¡Œ
        "éƒ½å¸‚ã®äº¤é€šæ¸‹æ»ã‚’è§£æ±ºã—ãªãŒã‚‰ç’°å¢ƒè² è·ã‚’æœ€å°åŒ–ã™ã‚‹ç·åˆçš„ãªäº¤é€šã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆ",
        
        # åˆ©å®³å¯¾ç«‹ã®èª¿æ•´ãŒå¿…è¦ãªå•é¡Œ  
        "é™ã‚‰ã‚ŒãŸæ°´è³‡æºã‚’è¾²æ¥­ã€å·¥æ¥­ã€ç”Ÿæ´»ç”¨æ°´ã«å…¬å¹³ã‹ã¤åŠ¹ç‡çš„ã«é…åˆ†ã™ã‚‹æ–¹æ³•",
        
        # å‰µé€ çš„è§£æ±ºãŒå¿…è¦ãªå•é¡Œ
        "é«˜é½¢åŒ–ç¤¾ä¼šã«ãŠã‘ã‚‹ä¸–ä»£é–“ã®çŸ¥è­˜ç¶™æ‰¿ã¨æŠ€è¡“é©æ–°ã‚’ä¸¡ç«‹ã•ã›ã‚‹ã‚·ã‚¹ãƒ†ãƒ ",
        
        # å€«ç†çš„åˆ¤æ–­ãŒå¿…è¦ãªå•é¡Œ
        "å€‹äººã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã¨å…¬å…±ã®å®‰å…¨ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹AIç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®è¨­è¨ˆ",
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªå”èª¿ãŒå¿…è¦ãªå•é¡Œ
        "å›½éš›çš„ãªç‚­ç´ æ’å‡ºæ¨©å–å¼•ã¨ç™ºå±•é€”ä¸Šå›½ã®çµŒæ¸ˆæˆé•·ã‚’ä¸¡ç«‹ã•ã›ã‚‹æ çµ„ã¿"
    ]
    
    # å„ã‚²ãƒ¼ãƒ ã‚¿ã‚¤ãƒ—ã§å®Ÿé¨“
    for game_type in GameType:
        await experiment.run_game_theory_interaction(agents, game_type, rounds=20)
    
    # çŸ¥è­˜å…±æœ‰ã‚»ãƒƒã‚·ãƒ§ãƒ³
    knowledge_topics = [
        "åˆ†æ•£å‹æ„æ€æ±ºå®šã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–",
        "ç«¶äº‰ã¨å”åŠ›ã®ãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥",
        "å‰µç™ºçš„ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¿ƒé€²æ–¹æ³•"
    ]
    
    for topic in knowledge_topics:
        await experiment.run_knowledge_sharing_session(agents, coordinator, topic)
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å•é¡Œã‚’è§£æ±º
    for problem in benchmark_problems:
        result = await experiment.run_complex_problem_solving(agents, coordinator, problem)
        
        # å˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚‚åŒã˜å•é¡Œã‚’è§£ã‹ã›ã¦æ¯”è¼ƒ
        single_agent = agents[0]  # ä»£è¡¨ã¨ã—ã¦1ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
        prompt = f"å•é¡Œ: {problem}\n\nã“ã®å•é¡Œã«å¯¾ã™ã‚‹è§£æ±ºç­–ã‚’ææ¡ˆã—ã¦ãã ã•ã„ã€‚"
        
        from agents import Runner, RunConfig
        runner = Runner()
        single_result = await runner.run(single_agent, prompt, config=RunConfig(
            max_token_count=500,
            save_sensitive_data=False
        ))
        
        print(f"\n  å˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ vs ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ:")
        print(f"  å‰µç™ºæ€§ã‚¹ã‚³ã‚¢: {result['evaluation']['emergence_score']:.2f}")
        print(f"  å˜ä¸€è§£ã®é•·ã•: {len(single_result.final_output)}")
        print(f"  çµ±åˆè§£ã®é•·ã•: {len(result['emergent_solution'])}")
    
    experiment._save_results()
    print("âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“å®Œäº†")


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    # ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
    load_dotenv()
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ ã‚¨ãƒ©ãƒ¼: OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("  .env ãƒ•ã‚¡ã‚¤ãƒ«ã« OPENAI_API_KEY=sk-... ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        return
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(description='OpenAI Agents SDK ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿé¨“')
    
    subparsers = parser.add_subparsers(dest='mode', help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
    
    # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰
    demo_parser = subparsers.add_parser('demo', help='ç°¡å˜ãªãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ')
    
    # ãƒ•ãƒ«å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰
    full_parser = subparsers.add_parser('full', help='å®Œå…¨ãªå®Ÿé¨“ã‚’å®Ÿè¡Œ')
    full_parser.add_argument('--name', type=str, default='å‰µç™ºçš„å•é¡Œè§£æ±ºå®Ÿé¨“',
                            help='å®Ÿé¨“å')
    
    # ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ãƒ¢ãƒ¼ãƒ‰
    custom_parser = subparsers.add_parser('custom', help='ã‚«ã‚¹ã‚¿ãƒ å®Ÿé¨“ã‚’å®Ÿè¡Œ')
    custom_parser.add_argument('--agents', type=int, default=3,
                              help='ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ•°')
    custom_parser.add_argument('--rounds', type=int, default=10,
                              help='ã‚²ãƒ¼ãƒ ãƒ©ã‚¦ãƒ³ãƒ‰æ•°')
    custom_parser.add_argument('--problems', nargs='+',
                              default=["æŒç¶šå¯èƒ½ãªç¤¾ä¼šã®å®Ÿç¾"],
                              help='è§£æ±ºã™ã‚‹å•é¡Œã®ãƒªã‚¹ãƒˆ')
    
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰
    bench_parser = subparsers.add_parser('benchmark', help='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ã‚’å®Ÿè¡Œ')
    
    args = parser.parse_args()
    
    # å®Ÿè¡Œ
    try:
        if args.mode == 'demo':
            asyncio.run(run_quick_demo())
        elif args.mode == 'full':
            experiment = MultiAgentExperiment(args.name)
            asyncio.run(experiment.run_full_experiment())
        elif args.mode == 'custom':
            asyncio.run(run_custom_experiment(
                args.agents, args.rounds, args.problems
            ))
        elif args.mode == 'benchmark':
            asyncio.run(run_benchmark_experiment())
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ•ãƒ«å®Ÿé¨“
            experiment = MultiAgentExperiment("å‰µç™ºçš„å•é¡Œè§£æ±ºå®Ÿé¨“")
            asyncio.run(experiment.run_full_experiment())
            
    except KeyboardInterrupt:
        print("\nâš ï¸ å®Ÿé¨“ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()